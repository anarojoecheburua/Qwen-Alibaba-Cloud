{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen - Getting Started and Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qwen is a series of powerful models trained on massive multilingual and multimodal datasets.  Developed by Alibaba Cloud, Qwen is pushing AI to new levels, making it smarter and more useful for all natural language processing, computer vision, and audio understanding.\n",
    "\n",
    "\n",
    "\n",
    "These models are capable of performing a wide range of tasks, including:\n",
    "Text generation and understanding\n",
    "Question answering\n",
    "Image captioning and analysis\n",
    "Visual question answering\n",
    "Audio processing\n",
    "Tool use and task planning\n",
    "\n",
    "The Qwen models are pre-trained on diverse data sources and further refined through post-training on high-quality data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will:\n",
    "\n",
    "1. Explore the key features of Qwen models, including their multilingual capabilities and multimodal processing abilities.\n",
    "2. Guide you through the process of accessing and installing Qwen models.\n",
    "3. Demonstrate practical examples of using Qwen for tasks such as text generation and question answering.\n",
    "4. Explore fine-tuning Qwen models on custom datasets for specialized applications.\n",
    "\n",
    "By the end of this tutorial, you'll have a comprehensive understanding of Qwen's capabilities and how to use it for your own projects and research.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation Steps and Getting Started\n",
    "This section, I will guide you through the process of using the Qwen-7B language model via Hugging Face. We'll cover setting up your environment, logging in to Hugging Face, and running the model.\n",
    "\n",
    "### Prerequisites\n",
    "- Python 3.7 or later\n",
    "- pip (Python package installer)\n",
    "\n",
    "### Step 1: Install Required Libraries\n",
    "First, install the necessary libraries:\n",
    "\n",
    "```\n",
    "pip install transformers torch huggingface_hub\n",
    "```\n",
    "### Step 2: Log in to Hugging Face\n",
    "To access the Qwen-7B model, you need to log in to your Hugging Face account. This is necessary because some models might have usage restrictions, but you can still access private models belonging to yourself or your organization. This helps in monitoring API usage and implementing rate limits if necessary.\n",
    "\n",
    "To log in, follow these steps:\n",
    "\n",
    "1. Go to Hugging Face\n",
    "2. Go to your profile settings and create an access token.\n",
    "3. In your terminal, run:\n",
    "```\n",
    "huggingface-cli login\n",
    "```\n",
    "\n",
    "4. When prompted, enter your access token.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Starter\n",
    "\n",
    "### Import Packages\n",
    "Now, we will create a Python file or Jupyter Notebook file where we will start writing our code.\n",
    "\n",
    "First, we will import two classes from the `transformers` library:\n",
    "`AutoModelForCausalLM`: This class automatically selects the appropriate model architecture based on the model name you provide.\n",
    "`AutoTokenizer`: This class loads the correct tokenizer for the model you're using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the Model Name\n",
    "Now, we're specifying which model we want to use. \"Qwen/Qwen-7B\" refers to the 7 billion parameter version of the Qwen model hosted on the Hugging Face model hub, but feel free to use other Qwen models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen-7B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Tokenizer\n",
    "Next, we need to load the tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line loads the tokenizer associated with the Qwen-7B model. The `trust_remote_code=True` parameter is necessary because Qwen models may require custom code to be executed during loading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model\n",
    "The next step is to load the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loads the actual Qwen-7B model. Again, we use `trust_remote_code=True` to allow any custom code associated with the model to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Test\n",
    "Once we have loaded the model, we can test if it works or not by generating some text given an input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Once upon a time\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we're preparing the input for the model:\n",
    "- We define a string `input_text` as our prompt.\n",
    "- We use the tokenizer to convert this text into a format the model can understand. The `return_tensors=\"pt\"` argument specifies that we want the output in PyTorch tensor format.\n",
    "\n",
    "Now, we generate the text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line of code tells the model to generate text:\n",
    "-We pass our tokenized `inputs` to the `generate` method.\n",
    "-`**inputs` unpacks the dictionary returned by the tokenizer.\n",
    "-`max_new_tokens=50` limits the generation to a maximum of 50 new tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step converts the model's output (which is in token ID format) back into readable text:\n",
    "-`outputs` selects the first (and in this case, only) generated sequence.\n",
    "-`skip_special_tokens=True` tells the decoder to ignore special tokens like padding or end-of-sequence tokens.\n",
    "\n",
    "Finally, we print the generated text to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes and Tips\n",
    "1. The Qwen-7B model is large (7 billion parameters). Ensure your system has sufficient RAM and preferably a GPU for faster processing. \n",
    "2. If you encounter memory issues, consider using a smaller model.\n",
    "3. The `trust_remote_code=True` parameter is necessary for Qwen models as they require custom code to run properly.\n",
    "4. Always review the model's license and usage restrictions on its Hugging Face page.\n",
    "\n",
    "You have successfully set up and run the Qwen model! Feel free to experiment with different input texts and explore the capabilities of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "Let's explore some practical examples of using Qwen for text generation and question answering tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation\n",
    "\n",
    "Qwen is really good at generating coherent and contextually relevant text based on given prompts. Letâ€™s look at some examples:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Text Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The future of artificial intelligence is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs, max_new_tokens=50)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creative Writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a short poem about the changing seasons:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs, max_new_tokens=100, temperature=0.7)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a Python function to calculate the Fibonacci sequence:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs, max_new_tokens=200, temperature=0.2)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answering\n",
    "Qwen can be used to answer a wide range of questions, from factual queries to more open-ended or analytical questions. Here are some examples:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Factual Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the capital of France?\"\n",
    "inputs = tokenizer(question, return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs, max_new_tokens=50)\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Q: {question}\\nA: {answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open-ended Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the potential ethical concerns surrounding artificial intelligence?\"\n",
    "inputs = tokenizer(question, return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs, max_new_tokens=200, temperature=0.7)\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Q: {question}\\nA: {answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These examples demonstrate how Qwen can handle various types of text generation and question answering tasks. You can adjust the parameters like `max_new_tokens` and `temperature`, in order to control the length and creativity of the generated responses to suit your specific needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usage and Fine-tuning\n",
    "Fine-tuning Qwen models allows you to adapt them to specific tasks, potentially improving their performance for your particular use case. This process involves training the pre-trained model on a custom dataset, allowing it to learn task-specific knowledge while retaining its general language understanding capabilities.\n",
    "\n",
    "In this section, we'll walk through the process of fine-tuning the Qwen-7B model on a custom dataset. We'll use efficient fine-tuning techniques to make this process manageable even for large models.  In our example, we're fine-tuning the model to improve its performance on translation tasks and answering factual questions. This process allows the model to learn from a custom dataset while retaining its general language understanding capabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "Before we begin, make sure you have the following installed:\n",
    "\n",
    "```\n",
    "pip install datasets torch accelerate peft\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Prepare Your Dataset\n",
    "\n",
    "First, prepare your dataset in a JSON format. Each entry should have a \"prompt\" and a \"completion\" field. Save this as `custom_dataset.json`. We will use this example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prompt': \"Translate to French: 'Hello, how are you?'\",\n",
       "  'completion': 'Bonjour, comment allez-vous?'},\n",
       " {'prompt': 'What is the capital of Spain?',\n",
       "  'completion': 'The capital of Spain is Madrid.'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "  {\n",
    "    \"prompt\": \"Translate to French: 'Hello, how are you?'\",\n",
    "    \"completion\": \"Bonjour, comment allez-vous?\"\n",
    "  },\n",
    "  {\n",
    "    \"prompt\": \"What is the capital of Spain?\",\n",
    "    \"completion\": \"The capital of Spain is Madrid.\"\n",
    "  }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Set Up the Fine-tuning Script\n",
    "\n",
    "We now add the following imports:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Prepare the Dataset\n",
    "Next, we'll define a function to preprocess our dataset and load it using the Hugging Face datasets library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [f\"{prompt}\\n\" for prompt in examples[\"prompt\"]]\n",
    "    targets = [f\"{completion}\\n\" for completion in examples[\"completion\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"custom_dataset.json\")\n",
    "tokenized_dataset = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "tokenized_dataset = tokenized_dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Set up LoRA for Efficient Fine-tuning\n",
    "\n",
    "To make fine-tuning more efficient, we'll use LoRA (Low-Rank Adaptation). This technique allows us to fine-tune large models with fewer parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Define Training Arguments\n",
    "Now, we'll set up the training arguments that control various aspects of the fine-tuning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_accumulation_steps=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Create Trainer and Start Fine-tuning\n",
    "With our dataset and training arguments prepared, we can now create a Trainer object and start the fine-tuning process:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Save the Fine-tuned Model\n",
    "After fine-tuning is complete, we'll save our model so we can use it later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./fine_tuned_qwen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning the Qwen-7B model on your custom dataset, will improve its performance on tasks similar to those in your training data. In this case, the model should become better at translating English to French and answering factual questions about capital cities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Fine-tuned Model\n",
    "\n",
    "After fine-tuning, you can use the model for inference:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_qwen\", trust_remote_code=True)\n",
    "fine_tuned_tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_qwen\", trust_remote_code=True)\n",
    "\n",
    "def generate_response(prompt):\n",
    "    inputs = fine_tuned_tokenizer(prompt, return_tensors=\"pt\").to(fine_tuned_model.device)\n",
    "    outputs = fine_tuned_model.generate(**inputs, max_new_tokens=50)\n",
    "    return fine_tuned_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Translate to French: 'Good morning, have a nice day!'\"\n",
    "response = generate_response(prompt)\n",
    "print(f\"Prompt: {prompt}\\nResponse: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips for Effective Fine-tuning\n",
    "\n",
    "- Ensure your custom dataset is high-quality and representative of the task you're targeting.\n",
    "- While larger datasets generally yield better results, even a few hundred examples can be beneficial for specific tasks.\n",
    "- Experiment with different learning rates, batch sizes, and number of epochs to optimize performance.\n",
    "- Regularly evaluate your model on a held-out validation set to prevent overfitting.\n",
    "- If you're working with limited GPU memory, consider using gradient accumulation to simulate larger batch sizes.\n",
    "- Use mixed precision training to speed up the fine-tuning process and reduce memory usage.\n",
    "- For domain-specific tasks, consider continued pre-training on domain-specific data before fine-tuning on your task-specific dataset.\n",
    "\n",
    "Fine-tuning Qwen on your custom dataset allows you to create a specialized model that excels at your specific task while retaining the broad knowledge and capabilities of the original pre-trained model. This approach uses the power of large language models for your unique applications and domains.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
